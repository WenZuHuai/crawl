中证服务器配置说明手册：

版本：v1.0
更新时间：2016.5.30

操作流程简介：
1）导入企业名单数据
2）导入生成器脚本及配置脚本参数
3）配置任务解析器，解析下载数据
4）配置任务导出器，mongo数据导出到mysql参数设置
5）配置系统任务crontab定时参数
6）查看mongo数据及日志，mysql数据



一.配置任务数据源

导入工商企业数据源到MySQL数据库

1.登陆本地Linux系统，通过Scp传输文件到阿里云-跳板机，传输目录为 /media/company

	a. 服务器存放目录：/media/company/enterprise-list.csv

	b. scp上传数据源到跳板机，再登陆跳板机上传到生成器。

	代码操作：
	scp [filename]  root@112.74.184.173:/media/company/[filename]
	scp [filename]  root@172.16.80.4:/media/company/[filename]
	
	代码示例：
	scp enterprise-list-9p.csv  root@112.74.184.173:/media/company/enterprise-list-9p.csv
        scp /media/company/enterprise-list.csv root@172.16.80.4:/media/company/enterprise-list.csv

	c. 执行命令，导入csv文件数据,(登陆生成器机器操作)：
	python /home/webapps/cr-clawer/clawer/manage_production.py import_enterprise_from_file_slice /media/company/[filename]

	d.输出结果示例:	

	line_count = 1000.
	line_count = 2000.
	..................

	line_count = 221000.
	line_count = 222000.
	{'is_ok': True, 'line_count': 222327, 'multiple': 51, 'success': 222161, 'failed': 166}

	e.参数说明：
	'is_ok': True           执行完毕，执行结果True为成功，False为失败。
	'line_count': 222327    文档的数据总数：222327行数据
	'multiple': 51	        文档中的重复数：51行
	'success': 222161       数据库导入成功：222161行
	'failed': 166           数据库导入失败：166行

	f.数据库数据验证（数据保存位置clawer的enterprise_enterprise表中）：

	启动MySQL数据库，终端中输入命令： mysql -uroot -p
	显示所有数据库： show databases；
	使用clawer：	use clawer
	查看数据条数：   select count(*) from enterprise_enterprise；
	查看详细数据：   select * from enterprise_enterprise limit 100;
	
	g.问题及异常：

	Q：数据库中查看数据时，中文为乱码，英文正常。
	A：数据库编码错误，使用 show variables like 'char%';查看除character_set_filesystem=binary 外是否都为utf8；若不是需要对mysql进行重新设置。


二.配置任务生成器脚本

导入脚本到任务数据库，等待任务调度

1.生成器脚本存放目录为/home/webapps/cr-clawer/clawer/collector/script/production
	
	a.脚本说明：
	enterprise_mysql_pro.py	: 工商数据uri生成器脚本
	baidu_pro.py            : 百度搜索uri生成器脚本
	simuwang_pro.py         : 私募网uri生成器脚本
	
	b.导入任务脚本到任务数据库：
	登陆生成器服务器，进入/home/webapps/cr-clawer/clawer目录：
	
	逐条执行导入代码
	参数说明：1）'* * * * *'为定时频率，默认为每1分钟执行一次，根据需求进行设定。
	         2）["enterprise"] 为设定uri格式，设定格式为：

		    ["enterprise"]:工商网站uri格式设定
		    ["http"]:为百度搜索uri格式设定
				
	工商生成器脚本导入代码及导出器导入配置如下：

	python manage_production.py shell
	from collector.tests.test_generator import insert_script_without_job

	file = '/home/webapps/cr-clawer/clawer/collector/script/production/enterprise_mysql_pro.py'
	script = open(file).read()
	settings={'cron':'*/10 * * * *' , 'schemes':["enterprise"], 'code_type': 1}

	insert_script_without_job(script, settings, extracter_conf)
        
        c.验证生成器任务导入情况：
	登陆mongodb并选择source数据库，查询任务情况：
	代码实例：
	mongo --host dds-wz9a828f745eac341.mongodb.rds.aliyuncs.com --port 3717 --authenticationDatabase admin -u root -p Password123
	use source
	db.crawler_download.count()
	db.crawler_download_setting.count()
	db.crawler_download_type.count()
	db.crawler_task_generator.count()
	db.job.count()
	
	根据查询结果，以上collections中各新增了一条任务记录，即任务导入成功。


三.配置任务解析器

配置解析器脚本，解析下载数据到mongo数据库，使用默认设置解析脚本	

四.配置任务导出器
    a. 导出器配置文件放置在目录: /home/webapps/cr-clawer/clawer/structure/extracter/
	b.配置文件说明：
        工商配置文件已经配置好, 文件名为conf_csciwlpc.json, 后期若需导出其他网站，应参照模板，遵循配置文件的格式进行配置，并放在上文提到的导出器配置文件目录，规则如下:
        1. 按模板格式修改需自定义的部分, 包括"数据库设置 database", "映射设置 mapping", "数据表设置 table"三个部分
        2. 关于 database 部分
         目标数据库现只支持 MySQL
        3. 关于 mapping 部分: 
            指定JSON源数据和关系数据库字段的映射关系和中英文表名的对应关系
        - dest_table 自定义目标数据库表的英文名称
        - source_path 表名在 JSON 源数据中的路径. 以列表形式呈现, 根据源数据的嵌套层次, 依次写入列表
        - associated_field_source_path 关联字段的在 JSON 源数据中的路径. 
        - dest_field 关系数据库表中字段的定义 
        4. 关于 table 部分:
        - dest_field 为 mapping 中相应的目的数据库表的字段
        - option 为字段的选项, 如设置主键, 默认值等
    c. 配置文件使用限制
        1. 仅能用于建立基本表
        2. associated_field_source_path 必须是 source_path 的子集
        3. 每个表必须设置 associated_field_source_path, 如不必要则填为空串(如 [""])
        4. 当配置文件改变，如，修改表字段，需要手动备份数据库


五.ip代理设置

购买ip代理账号，配置ip自动抓取脚本		

	a.ip代理网站使用666代理网站，注册购买专业版包月ip，获取并保存订单号。
	
	b.在Webserver服务器上，在/home/webapps/cr-clawer/clawer/clawer/settings/production.py中配置：

	  ipuser_tid = 订单号

	c.并及时确认每月的续费情况，保证系统正常运行。

六.配置系统定时设置启动任务

配置linux Crontab，定时启动生成器，下载器，解析器任务

	a.登陆WebServer服务器，终端中输入crontab -l查看定时任务：

	示例：
	*/10    *    *    *    * cd /home/webapps/cr-clawer/confs/cr;./bg_cmd.sh generator_install
	*/1     *    *    *    * cd /home/webapps/cr-clawer/confs/cr;./bg_cmd.sh generator_dispatch
	*/5     *    *    *    * cd /home/webapps/cr-clawer/confs/cr;./bg_cmd.sh downloader_dispatch
	*/1     *    *    *    * cd /home/webapps/cr-clawer/confs/cr;./bg_cmd.sh task_parser_dispatch
	*/1     *    *    *    * cd /home/webapps/cr-clawer/confs/cr;./bg_cmd.sh task_extracter_dispatch
	
	b.参数说明：
	*/10 * * * * ：设定为每10十分钟执行一次该命令
	generator_install           ：为任务定时检测，每10分钟定时检测是否有新的任务加入，并初始化任务
	generator_dispatch          ：为生成器任务定时分发，每1分钟执行一次任务分发（不需要改动）
	downloader_dispatch         ：为下载器任务定时分发，每5分钟执行一次分发。
	task_parser_dispatch        ：为解析器任务定时分发，每分钟执行一次分发。
	task_extracter_dispatch     ：为解析器任务定时导入数据库，每分钟执行一次分发。

	c.根据任务的执行速度，需合理配置定时分发频率。
	
	d.输入cronta -e编辑定时任务，保存即可运行。

七.查看任务rqworker运行状态

通过rqworker执行任务，通过rq info查看任务运行状态，包括worker数量，状态，队列状态等

1.查看rq info：
	
	a.rq info命令及参数说明：
	
	rq info  --url redis://:Password123@13153c2b13894978.m.cnsza.kvstore.aliyuncs.com/1 
	rq info  --url redis://:Password123@13153c2b13894978.m.cnsza.kvstore.aliyuncs.com/2
	rq info  --url redis://:Password123@13153c2b13894978.m.cnsza.kvstore.aliyuncs.com/3
	rq info  --url redis://:Password123@13153c2b13894978.m.cnsza.kvstore.aliyuncs.com/5

	尾号1对应生成器状态
	尾号2对应下载器状态
	尾号3对应解析器状态
	尾号5对应导出器状态

2.查看mongodb数据库数据：
	
	a.mongodb数据查看
	在跳板机登陆mongo客户端：
	登陆命令：
	mongo --host dds-wz9a828f745eac341.mongodb.rds.aliyuncs.com --port 3717 --authenticationDatabase admin -u root -p Password123
	
	b.use source 选择source数据库-
	查看crawler_download_data集合数据情况，即下载的工商网站源数据。

	c.mongodb数据库结构：
	
	source：crawler_download               ：下载器任务
		crawler_download_data          ：任务下载的源数据
		crawler_download_setting       ：下载器任务的设置
		crawler_download_type          ：下载器任务的类型
		crawler_task                   ：下载器任务的uri源，也是生成器生成的uri数据
		crawler_task_generator         ：生成器任务
		job                            ：整体云爬虫的任务job，是下载器，生成器，解析器等识别多个任务的标识。
					         多个任务之间依靠job的id进行识别，如工商任务，百度任务的识别标识。
	log    ：crawler_dispatch_alert_log    ：下载器任务分发日志
		 crawler_download_log          ：下载器下载日志
		 crawler_generator_alert_log   ：生成器警告日志
		 crawler_generator_cron_log    ：生成器新任务初始化日志
		 crawler_generator_dispatch_log：生成器分发日志
		 crawler_generator_error_log   ：生成器错误日志
		 crawler_generator_log         ：生成器生成日志

2.查看MySQL数据库数据：

	工商信息数据库结构：

	Basi			：企业基本信息
	IndustryCommerceMainperson	：企业主要员工信息
	IndustryCommerceBranch		：企业登记信息
	EnterAnnualReport			：年报基本信息
	YearReportAssets			：年报-企业资产状况信息
	YearReportSharechange		：年报-股权变更信息
	YearReportOnline			：年报-企业网站或网店信息
	YearReportInvestment		：年报-对外投资信息
	YearReportModify			：年报-修改记录
	YearReportWarrandice		：年报-对外提供保证担保信息
	YearReportShareholder		：年报-股东及出资信息
	YearReportBasic				：年报-企业基本信息
	ind_comm_pub_reg_shareholder	：工商公示-投资人信息明细
	ind_comm_pub_reg_modify			：工商公示-企业变更信息
	ind_comm_pub_arch_liquidation	：工商公示-清算信息
	ind_comm_pub_movable_property_reg		：工商公示-动产抵押登记信息
	ind_comm_pub_equity_ownership_reg		：工商公示-知识产权出质登记信息
	ind_comm_pub_administration_sanction	：工商公示-行政处罚信息
	ind_comm_pub_business_exception			：工商公示-经营异常信息
	ind_comm_pub_serious_violate_law		：工商公示-严重违法失信信息
	ind_comm_pub_spot_check					：工商公示-抽查检查信息
	ent_pub_shareholder_capital_contribution：企业公示-投资人及出资信息
	ent_pub_equity_change			：企业公示-股权变更信息
	ent_pub_administration_license	：企业公示-行政许可信息
	ent_pub_knowledge_property		：企业公示-知识产权出质登记信息
	ent_pub_administration_sanction	：企业公示-行政处罚信息
	other_dept_pub_administration_license	：其他部门公示-行政许可信息
	other_dept_pub_administration_sanction	：其他部门公示-行政处罚信息
	judical_assist_pub_equity_freeze		：司法协助公示-股权冻结信息
	judical_assist_pub_shareholder_modify	：司法协助公示-股权变更信息
	



























































